# Thesis Results Mapping

This document maps the experimental results to their corresponding thesis sections.

## Source of Truth

**Canonical Script**: `thesis_experiments.py`  
**Official Results**: `THESIS_OFFICIAL_RESULTS.txt` (generated by running `thesis_experiments.py`)

## Experiment Mapping

### Experiment 1: Scalability Analysis
- **Thesis Section**: Chapter 4.1 or Results Section 1
- **What It Measures**: Encryption + indexing performance across batch sizes (1K, 5K, 10K patients)
- **Key Metrics**:
  - Total encryption time (seconds)
  - Average time per record (milliseconds)
  - Peak RAM usage (MB)
- **Reported Numbers**: From `THESIS_OFFICIAL_RESULTS.txt` → Experiment 1 section

### Experiment 2: Keyword Density Impact
- **Thesis Section**: Chapter 4.2 or Results Section 2
- **What It Measures**: **ISOLATED DSSE performance** (index build + search)
- **Key Metrics**:
  - Index build time (milliseconds)
  - **Search time (milliseconds)** ← This is the **0.002-0.064ms** range (pure DSSE)
- **Reported Numbers**: From `THESIS_OFFICIAL_RESULTS.txt` → Experiment 2 section
- **IMPORTANT**: This measures **ONLY** DSSE hash lookup, NOT the full workflow

### Experiment 3: Resource Constraints
- **Status**: **OPTIONAL** - Not included in `thesis_experiments.py`
- **Reason**: Can be tested manually with Docker `--memory` and `--cpus` flags
- **If Needed**: Run `thesis_experiments.py` with Docker resource limits

### Experiment 4: Concurrent Query Workload
- **Thesis Section**: Chapter 4.3 or Results Section 3
- **What It Measures**: **FULL WORKFLOW** (CP-ABE decrypt + DSSE search) under concurrent load
- **Key Metrics**:
  - Throughput (queries/second)
  - **Average latency (milliseconds)** ← This is the **7-12ms** range (full workflow)
- **Reported Numbers**: From `THESIS_OFFICIAL_RESULTS.txt` → Experiment 4 section
- **IMPORTANT**: This measures the **COMPLETE** workflow including CP-ABE overhead

### Experiment 5: Baseline Comparison
- **Thesis Section**: Chapter 4.4 or Results Section 4
- **What It Measures**: Per-patient indexing vs. global-index ABSE
- **Key Metrics**:
  - Our system search latency (ms)
  - Baseline search latency (ms)
  - Performance improvement (%)
- **Reported Numbers**: From `THESIS_OFFICIAL_RESULTS.txt` → Experiment 5 section

### Experiment 6: Policy Complexity
- **Thesis Section**: Chapter 4.5 or Results Section 5
- **What It Measures**: CP-ABE overhead as policy complexity increases
- **Key Metrics**:
  - Encryption time (ms) per policy depth
  - Decryption time (ms) per policy depth
  - Average overhead per attribute (ms)
- **Reported Numbers**: From `THESIS_OFFICIAL_RESULTS.txt` → Experiment 6 section

## Clarifying the Search Time "Discrepancy"

### There is NO discrepancy - these measure different things:

| Metric | Value | Source | What It Measures |
|:---|:---|:---|:---|
| **DSSE Search** | 0.002-0.064 ms | Experiment 2 | Pure hash table lookup (DSSE only) |
| **Full Workflow** | 7-12 ms | Experiment 4 | CP-ABE decrypt + DSSE search + network |

### Why the 300× difference?

The full workflow includes:
1. **Fetch encrypted key** from server (~1ms network simulation)
2. **CP-ABE decryption** (~5-8ms - this is the expensive part)
3. **Generate DSSE trapdoor** (~0.1ms)
4. **Server-side DSSE search** (~0.002-0.064ms - from Experiment 2)
5. **Return results** (~1ms network simulation)

**Total**: ~7-12ms (as measured in Experiment 4)

## What to Report in Your Thesis

### For Search Performance:
- **Isolated DSSE**: "Search latency is 0.002-0.064ms for keyword densities of 100-20,000 keywords (Experiment 2)"
- **Full Workflow**: "End-to-end search latency including access control is 7-12ms under concurrent load (Experiment 4)"

### For Encryption Performance:
- "Average encryption time per patient record is ~11-12ms (Experiment 1)"

### For Scalability:
- "System scales linearly up to 10,000 patients with consistent per-record performance (Experiment 1)"

### For Access Control:
- "CP-ABE overhead is ~3-19ms per attribute depending on policy complexity (Experiment 6)"

## Files to Archive/Delete

### Archive (move to `archive/` folder):
- `detailed_evaluation.py`
- `detailed_evaluation_v2.py`
- `final_verified_evaluation.py`
- `dynamic_dsse_experiment.py`
- All `diagnostic_*.py`
- All `measure_*.py`
- All `proof_*.py`
- `run_exp4_corrected.py`
- `run_exp6_corrected.py`

### Delete (redundant result files):
- `final_results_dynamic.txt` (12MB - this was Experiment 4 output from old script)
- `experiment_results_*.txt` (multiple versions)
- All `*_corrected_*.txt`
- All `*_verified_*.txt`
- `CONCISE_DYNAMIC_RESULTS.md` (summary of wrong file)

### Keep:
- `thesis_experiments.py` ← **CANONICAL SCRIPT**
- `THESIS_OFFICIAL_RESULTS.txt` ← **OFFICIAL RESULTS** (generated)
- `ACTUAL_EXPERIMENTAL_RESULTS.txt` ← **REFERENCE** (your current thesis numbers)
- `FORMAL_EXPERIMENTAL_REPORT.txt` ← **REFERENCE** (detailed report)

## Next Steps

1. **Run the canonical script**:
   ```bash
   docker build -t hybrid-workflow .
   docker run --rm -v $(pwd):/app hybrid-workflow python thesis_experiments.py
   ```

2. **Compare outputs**:
   - Check if `THESIS_OFFICIAL_RESULTS.txt` matches `ACTUAL_EXPERIMENTAL_RESULTS.txt`
   - If discrepancies exist, investigate which measurement is correct

3. **Update thesis**:
   - Use numbers from `THESIS_OFFICIAL_RESULTS.txt`
   - Cite this mapping document for clarity

4. **Clean up**:
   - Archive old experimental scripts
   - Delete redundant result files
   - Reduce Docker image size
